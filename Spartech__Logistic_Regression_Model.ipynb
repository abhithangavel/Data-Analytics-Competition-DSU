{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import zscore\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "D4DbQB4LbRDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_pd = pd.read_csv('Nonull_Clean.csv')\n",
        "#print(data_pd)\n",
        "#data_pd.shape"
      ],
      "metadata": {
        "id": "iGwbflGHbcvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the features (x) and target variable (y) from the dataset\n",
        "x = data_pd.drop('Target1', axis=1)\n",
        "y = data_pd['Target1']\n",
        "\n",
        "# Initialize the StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the features and transform the data\n",
        "x_scaled = scaler.fit_transform(x)\n"
      ],
      "metadata": {
        "id": "-T3AaFYn_fVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(x_scaled)\n",
        "y = np.array(y)\n",
        "\n",
        "y_train, y_Val = train_test_split(y, train_size=0.8,random_state=40)"
      ],
      "metadata": {
        "id": "8rM3WYwH_0-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2,85):\n",
        "  pca = PCA(n_components = i)\n",
        "  data_reduced = pca.fit_transform(x_scaled)\n",
        "  model = LogisticRegression(max_iter=5000)\n",
        "  X_train, X_Val = train_test_split(data_reduced, train_size=0.8,random_state=42)\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred = model.predict(X_Val)\n",
        "  print('Accuracy for',i,':', accuracy_score(y_Val, y_pred))\n",
        "  print('F1 Score for' ,i, ':' ,f1_score(y_Val, y_pred))\n",
        "  print('Confusion Matrix for',i,':', confusion_matrix(y_Val, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwCJQ_r9LzqM",
        "outputId": "63d1db15-7d4a-4cfa-99fd-07cdce6a8f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 2 : 0.5006666666666667\n",
            "F1 Score for 2 : 0.5077226421294775\n",
            "Confusion Matrix for 2 : [[14590 15399]\n",
            " [14561 15450]]\n",
            "Accuracy for 3 : 0.49805\n",
            "F1 Score for 3 : 0.5133864374464785\n",
            "Confusion Matrix for 3 : [[13996 15993]\n",
            " [14124 15887]]\n",
            "Accuracy for 4 : 0.49851666666666666\n",
            "F1 Score for 4 : 0.5135873518808904\n",
            "Confusion Matrix for 4 : [[14026 15963]\n",
            " [14126 15885]]\n",
            "Accuracy for 5 : 0.49853333333333333\n",
            "F1 Score for 5 : 0.5136271054928712\n",
            "Confusion Matrix for 5 : [[14025 15964]\n",
            " [14124 15887]]\n",
            "Accuracy for 6 : 0.49773333333333336\n",
            "F1 Score for 6 : 0.5081763880275484\n",
            "Confusion Matrix for 6 : [[14295 15694]\n",
            " [14442 15569]]\n",
            "Accuracy for 7 : 0.49928333333333336\n",
            "F1 Score for 7 : 0.4838593296338928\n",
            "Confusion Matrix for 7 : [[15875 14114]\n",
            " [15929 14082]]\n",
            "Accuracy for 8 : 0.49978333333333336\n",
            "F1 Score for 8 : 0.4889577551124657\n",
            "Confusion Matrix for 8 : [[15629 14360]\n",
            " [15653 14358]]\n",
            "Accuracy for 9 : 0.49816666666666665\n",
            "F1 Score for 9 : 0.4908002435229656\n",
            "Confusion Matrix for 9 : [[15379 14610]\n",
            " [15500 14511]]\n",
            "Accuracy for 10 : 0.4993\n",
            "F1 Score for 10 : 0.49024332303933216\n",
            "Confusion Matrix for 10 : [[15512 14477]\n",
            " [15565 14446]]\n",
            "Accuracy for 11 : 0.4984\n",
            "F1 Score for 11 : 0.49375946173254837\n",
            "Confusion Matrix for 11 : [[15227 14762]\n",
            " [15334 14677]]\n",
            "Accuracy for 12 : 0.5016666666666667\n",
            "F1 Score for 12 : 0.4843760778092019\n",
            "Confusion Matrix for 12 : [[16056 13933]\n",
            " [15967 14044]]\n",
            "Accuracy for 13 : 0.4998666666666667\n",
            "F1 Score for 13 : 0.48450491307634164\n",
            "Confusion Matrix for 13 : [[15890 14099]\n",
            " [15909 14102]]\n",
            "Accuracy for 14 : 0.50055\n",
            "F1 Score for 14 : 0.4834789802988779\n",
            "Confusion Matrix for 14 : [[16008 13981]\n",
            " [15986 14025]]\n",
            "Accuracy for 15 : 0.5023333333333333\n",
            "F1 Score for 15 : 0.4843366835906469\n",
            "Confusion Matrix for 15 : [[16117 13872]\n",
            " [15988 14023]]\n",
            "Accuracy for 16 : 0.50155\n",
            "F1 Score for 16 : 0.4800139094149352\n",
            "Confusion Matrix for 16 : [[16289 13700]\n",
            " [16207 13804]]\n",
            "Accuracy for 17 : 0.49961666666666665\n",
            "F1 Score for 17 : 0.47904773472610235\n",
            "Confusion Matrix for 17 : [[16173 13816]\n",
            " [16207 13804]]\n",
            "Accuracy for 18 : 0.5000166666666667\n",
            "F1 Score for 18 : 0.4778514611943676\n",
            "Confusion Matrix for 18 : [[16274 13715]\n",
            " [16284 13727]]\n",
            "Accuracy for 19 : 0.50145\n",
            "F1 Score for 19 : 0.48736097067745204\n",
            "Confusion Matrix for 19 : [[15868 14121]\n",
            " [15792 14219]]\n",
            "Accuracy for 20 : 0.50235\n",
            "F1 Score for 20 : 0.48416688261207563\n",
            "Confusion Matrix for 20 : [[16128 13861]\n",
            " [15998 14013]]\n",
            "Accuracy for 21 : 0.50165\n",
            "F1 Score for 21 : 0.48290531776913104\n",
            "Confusion Matrix for 21 : [[16137 13852]\n",
            " [16049 13962]]\n",
            "Accuracy for 22 : 0.5008666666666667\n",
            "F1 Score for 22 : 0.482495247969587\n",
            "Confusion Matrix for 22 : [[16091 13898]\n",
            " [16050 13961]]\n",
            "Accuracy for 23 : 0.50165\n",
            "F1 Score for 23 : 0.4820992465575474\n",
            "Confusion Matrix for 23 : [[16182 13807]\n",
            " [16094 13917]]\n",
            "Accuracy for 24 : 0.50185\n",
            "F1 Score for 24 : 0.484645757539183\n",
            "Confusion Matrix for 24 : [[16057 13932]\n",
            " [15957 14054]]\n",
            "Accuracy for 25 : 0.5016833333333334\n",
            "F1 Score for 25 : 0.4832258845083568\n",
            "Confusion Matrix for 25 : [[16122 13867]\n",
            " [16032 13979]]\n",
            "Accuracy for 26 : 0.50285\n",
            "F1 Score for 26 : 0.4855206195346591\n",
            "Confusion Matrix for 26 : [[16096 13893]\n",
            " [15936 14075]]\n",
            "Accuracy for 27 : 0.5016833333333334\n",
            "F1 Score for 27 : 0.4855732007363948\n",
            "Confusion Matrix for 27 : [[15990 13999]\n",
            " [15900 14111]]\n",
            "Accuracy for 28 : 0.5022333333333333\n",
            "F1 Score for 28 : 0.4880172798025165\n",
            "Confusion Matrix for 28 : [[15900 14089]\n",
            " [15777 14234]]\n",
            "Accuracy for 29 : 0.5023833333333333\n",
            "F1 Score for 29 : 0.4866314757819082\n",
            "Confusion Matrix for 29 : [[15992 13997]\n",
            " [15860 14151]]\n",
            "Accuracy for 30 : 0.50195\n",
            "F1 Score for 30 : 0.4901816941056044\n",
            "Confusion Matrix for 30 : [[15751 14238]\n",
            " [15645 14366]]\n",
            "Accuracy for 31 : 0.5028666666666667\n",
            "F1 Score for 31 : 0.48722709300326633\n",
            "Confusion Matrix for 31 : [[16001 13988]\n",
            " [15840 14171]]\n",
            "Accuracy for 32 : 0.50195\n",
            "F1 Score for 32 : 0.4841087613293052\n",
            "Confusion Matrix for 32 : [[16096 13893]\n",
            " [15990 14021]]\n",
            "Accuracy for 33 : 0.5017166666666667\n",
            "F1 Score for 33 : 0.4822223376803312\n",
            "Confusion Matrix for 33 : [[16181 13808]\n",
            " [16089 13922]]\n",
            "Accuracy for 34 : 0.50185\n",
            "F1 Score for 34 : 0.4813368733406216\n",
            "Confusion Matrix for 34 : [[16242 13747]\n",
            " [16142 13869]]\n",
            "Accuracy for 35 : 0.5025833333333334\n",
            "F1 Score for 35 : 0.4843731103470914\n",
            "Confusion Matrix for 35 : [[16137 13852]\n",
            " [15993 14018]]\n",
            "Accuracy for 36 : 0.5026166666666667\n",
            "F1 Score for 36 : 0.48136111642133433\n",
            "Confusion Matrix for 36 : [[16308 13681]\n",
            " [16162 13849]]\n",
            "Accuracy for 37 : 0.5008666666666667\n",
            "F1 Score for 37 : 0.4804302567661346\n",
            "Confusion Matrix for 37 : [[16206 13783]\n",
            " [16165 13846]]\n",
            "Accuracy for 38 : 0.5011666666666666\n",
            "F1 Score for 38 : 0.4807065028801443\n",
            "Confusion Matrix for 38 : [[16217 13772]\n",
            " [16158 13853]]\n",
            "Accuracy for 39 : 0.5016666666666667\n",
            "F1 Score for 39 : 0.48474926762019643\n",
            "Confusion Matrix for 39 : [[16035 13954]\n",
            " [15946 14065]]\n",
            "Accuracy for 40 : 0.50215\n",
            "F1 Score for 40 : 0.48532883061389753\n",
            "Confusion Matrix for 40 : [[16045 13944]\n",
            " [15927 14084]]\n",
            "Accuracy for 41 : 0.5014333333333333\n",
            "F1 Score for 41 : 0.4845613067751051\n",
            "Confusion Matrix for 41 : [[16025 13964]\n",
            " [15950 14061]]\n",
            "Accuracy for 42 : 0.5010666666666667\n",
            "F1 Score for 42 : 0.48679969827881786\n",
            "Confusion Matrix for 42 : [[15866 14123]\n",
            " [15813 14198]]\n",
            "Accuracy for 43 : 0.50035\n",
            "F1 Score for 43 : 0.4864938935612613\n",
            "Confusion Matrix for 43 : [[15820 14169]\n",
            " [15810 14201]]\n",
            "Accuracy for 44 : 0.5008833333333333\n",
            "F1 Score for 44 : 0.4880942206116134\n",
            "Confusion Matrix for 44 : [[15776 14213]\n",
            " [15734 14277]]\n",
            "Accuracy for 45 : 0.49966666666666665\n",
            "F1 Score for 45 : 0.4840683325885952\n",
            "Confusion Matrix for 45 : [[15897 14092]\n",
            " [15928 14083]]\n",
            "Accuracy for 46 : 0.5001\n",
            "F1 Score for 46 : 0.48421378456458936\n",
            "Confusion Matrix for 46 : [[15927 14062]\n",
            " [15932 14079]]\n",
            "Accuracy for 47 : 0.4993666666666667\n",
            "F1 Score for 47 : 0.4827633708716465\n",
            "Confusion Matrix for 47 : [[15944 14045]\n",
            " [15993 14018]]\n",
            "Accuracy for 48 : 0.4991333333333333\n",
            "F1 Score for 48 : 0.4829674489023467\n",
            "Confusion Matrix for 48 : [[15912 14077]\n",
            " [15975 14036]]\n",
            "Accuracy for 49 : 0.49921666666666664\n",
            "F1 Score for 49 : 0.48226070474713534\n",
            "Confusion Matrix for 49 : [[15959 14030]\n",
            " [16017 13994]]\n",
            "Accuracy for 50 : 0.4991\n",
            "F1 Score for 50 : 0.48225606394707826\n",
            "Confusion Matrix for 50 : [[15949 14040]\n",
            " [16014 13997]]\n",
            "Accuracy for 51 : 0.49946666666666667\n",
            "F1 Score for 51 : 0.48283106595488207\n",
            "Confusion Matrix for 51 : [[15949 14040]\n",
            " [15992 14019]]\n",
            "Accuracy for 52 : 0.49935\n",
            "F1 Score for 52 : 0.48261251485557793\n",
            "Confusion Matrix for 52 : [[15951 14038]\n",
            " [16001 14010]]\n",
            "Accuracy for 53 : 0.4996\n",
            "F1 Score for 53 : 0.4853440296204875\n",
            "Confusion Matrix for 53 : [[15819 14170]\n",
            " [15854 14157]]\n",
            "Accuracy for 54 : 0.4990833333333333\n",
            "F1 Score for 54 : 0.4846271241661951\n",
            "Confusion Matrix for 54 : [[15814 14175]\n",
            " [15880 14131]]\n",
            "Accuracy for 55 : 0.4991833333333333\n",
            "F1 Score for 55 : 0.48473001011711847\n",
            "Confusion Matrix for 55 : [[15817 14172]\n",
            " [15877 14134]]\n",
            "Accuracy for 56 : 0.4988\n",
            "F1 Score for 56 : 0.4843444562570733\n",
            "Confusion Matrix for 56 : [[15805 14184]\n",
            " [15888 14123]]\n",
            "Accuracy for 57 : 0.4985833333333333\n",
            "F1 Score for 57 : 0.4831555257778007\n",
            "Confusion Matrix for 57 : [[15853 14136]\n",
            " [15949 14062]]\n",
            "Accuracy for 58 : 0.49861666666666665\n",
            "F1 Score for 58 : 0.48372204774408356\n",
            "Confusion Matrix for 58 : [[15824 14165]\n",
            " [15918 14093]]\n",
            "Accuracy for 59 : 0.49948333333333333\n",
            "F1 Score for 59 : 0.4832665141008655\n",
            "Confusion Matrix for 59 : [[15926 14063]\n",
            " [15968 14043]]\n",
            "Accuracy for 60 : 0.49955\n",
            "F1 Score for 60 : 0.4829794927423937\n",
            "Confusion Matrix for 60 : [[15948 14041]\n",
            " [15986 14025]]\n",
            "Accuracy for 61 : 0.5003166666666666\n",
            "F1 Score for 61 : 0.4836114986479271\n",
            "Confusion Matrix for 61 : [[15980 14009]\n",
            " [15972 14039]]\n",
            "Accuracy for 62 : 0.5002833333333333\n",
            "F1 Score for 62 : 0.48380821210295255\n",
            "Confusion Matrix for 62 : [[15966 14023]\n",
            " [15960 14051]]\n",
            "Accuracy for 63 : 0.4998166666666667\n",
            "F1 Score for 63 : 0.4832549890662397\n",
            "Confusion Matrix for 63 : [[15956 14033]\n",
            " [15978 14033]]\n",
            "Accuracy for 64 : 0.5000333333333333\n",
            "F1 Score for 64 : 0.48293574186431326\n",
            "Confusion Matrix for 64 : [[15993 13996]\n",
            " [16002 14009]]\n",
            "Accuracy for 65 : 0.5000333333333333\n",
            "F1 Score for 65 : 0.4826503863134658\n",
            "Confusion Matrix for 65 : [[16009 13980]\n",
            " [16018 13993]]\n",
            "Accuracy for 66 : 0.4998\n",
            "F1 Score for 66 : 0.48424127857020105\n",
            "Confusion Matrix for 66 : [[15899 14090]\n",
            " [15922 14089]]\n",
            "Accuracy for 67 : 0.49966666666666665\n",
            "F1 Score for 67 : 0.4841037979034199\n",
            "Confusion Matrix for 67 : [[15895 14094]\n",
            " [15926 14085]]\n",
            "Accuracy for 68 : 0.5004\n",
            "F1 Score for 68 : 0.48503693523449587\n",
            "Confusion Matrix for 68 : [[15907 14082]\n",
            " [15894 14117]]\n",
            "Accuracy for 69 : 0.5005\n",
            "F1 Score for 69 : 0.48363197794624396\n",
            "Confusion Matrix for 69 : [[15995 13994]\n",
            " [15976 14035]]\n",
            "Accuracy for 70 : 0.4998666666666667\n",
            "F1 Score for 70 : 0.481727115716753\n",
            "Confusion Matrix for 70 : [[16046 13943]\n",
            " [16065 13946]]\n",
            "Accuracy for 71 : 0.49956666666666666\n",
            "F1 Score for 71 : 0.4807705609739227\n",
            "Confusion Matrix for 71 : [[16073 13916]\n",
            " [16110 13901]]\n",
            "Accuracy for 72 : 0.49971666666666664\n",
            "F1 Score for 72 : 0.4796755013954134\n",
            "Confusion Matrix for 72 : [[16147 13842]\n",
            " [16175 13836]]\n",
            "Accuracy for 73 : 0.5001166666666667\n",
            "F1 Score for 73 : 0.480433765828815\n",
            "Confusion Matrix for 73 : [[16140 13849]\n",
            " [16144 13867]]\n",
            "Accuracy for 74 : 0.5007166666666667\n",
            "F1 Score for 74 : 0.4814526319433625\n",
            "Confusion Matrix for 74 : [[16136 13853]\n",
            " [16104 13907]]\n",
            "Accuracy for 75 : 0.5006\n",
            "F1 Score for 75 : 0.4813224857192314\n",
            "Confusion Matrix for 75 : [[16133 13856]\n",
            " [16108 13903]]\n",
            "Accuracy for 76 : 0.50015\n",
            "F1 Score for 76 : 0.48104376113927777\n",
            "Confusion Matrix for 76 : [[16109 13880]\n",
            " [16111 13900]]\n",
            "Accuracy for 77 : 0.4994166666666667\n",
            "F1 Score for 77 : 0.48227121507248377\n",
            "Confusion Matrix for 77 : [[15976 14013]\n",
            " [16022 13989]]\n",
            "Accuracy for 78 : 0.50115\n",
            "F1 Score for 78 : 0.48457922198687814\n",
            "Confusion Matrix for 78 : [[15999 13990]\n",
            " [15941 14070]]\n",
            "Accuracy for 79 : 0.5010833333333333\n",
            "F1 Score for 79 : 0.48703668797230837\n",
            "Confusion Matrix for 79 : [[15854 14135]\n",
            " [15800 14211]]\n",
            "Accuracy for 80 : 0.5009833333333333\n",
            "F1 Score for 80 : 0.4870393530812589\n",
            "Confusion Matrix for 80 : [[15845 14144]\n",
            " [15797 14214]]\n",
            "Accuracy for 81 : 0.5011833333333333\n",
            "F1 Score for 81 : 0.48722737163123\n",
            "Confusion Matrix for 81 : [[15852 14137]\n",
            " [15792 14219]]\n",
            "Accuracy for 82 : 0.5011833333333333\n",
            "F1 Score for 82 : 0.48722737163123\n",
            "Confusion Matrix for 82 : [[15852 14137]\n",
            " [15792 14219]]\n",
            "Accuracy for 83 : 0.5009333333333333\n",
            "F1 Score for 83 : 0.4870846180198698\n",
            "Confusion Matrix for 83 : [[15838 14151]\n",
            " [15793 14218]]\n",
            "Accuracy for 84 : 0.5009333333333333\n",
            "F1 Score for 84 : 0.4870846180198698\n",
            "Confusion Matrix for 84 : [[15838 14151]\n",
            " [15793 14218]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_pd_cp = data_pd.copy()\n",
        "\n",
        "# Standardize the data using z-scores\n",
        "data_z = data_pd_cp.apply(zscore)\n",
        "\n",
        "# Calculate the absolute z-score for each data point\n",
        "data_z_abs = data_z.abs()\n",
        "\n",
        "# Identify the outliers based on a z-score threshold\n",
        "outliers = (data_z_abs > 3).any(axis=1)\n",
        "\n",
        "# Print the indices of the outliers\n",
        "outlier_indices = (outliers.index[outliers])\n",
        "\n",
        "data_pd_cp = data_pd_cp.drop(outlier_indices)\n",
        "\n",
        "# Extract the features (x) and target variable (y) from the dataset\n",
        "x = data_pd_cp.drop('Target1', axis=1)\n",
        "y = data_pd_cp['Target1']\n",
        "\n",
        "# Initialize the StandardScaler object\n",
        "#scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the features and transform the data\n",
        "#x_scaled = scaler.fit_transform(x)\n",
        "acc_best = 0\n",
        "f1_best = 0\n",
        "\n",
        "for i in range(2,85):\n",
        "  pca = PCA(n_components = i)\n",
        "  data_reduced = pca.fit_transform(x)\n",
        "  model = LogisticRegression(max_iter=50000)\n",
        "  X_train, X_Val, y_train, y_Val = train_test_split(data_reduced,y, train_size=0.8,random_state=42)\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred = model.predict(X_Val)\n",
        "  acc = accuracy_score(y_Val, y_pred)\n",
        "  f1 = f1_score(y_Val, y_pred)\n",
        "  if acc > acc_best :\n",
        "    acc_best = acc\n",
        "    print('Accuracy Best',i)\n",
        "  if f1 > f1_best:\n",
        "    f1_best = f1\n",
        "    print('F1 Best',i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "pm1um52wdoxb",
        "outputId": "b8b8be5d-a889-42bf-e703-95274a234c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Best 2\n",
            "F1 Best 2\n",
            "F1 Best 3\n",
            "Accuracy Best 4\n",
            "F1 Best 4\n",
            "Accuracy Best 5\n",
            "F1 Best 5\n",
            "Accuracy Best 6\n",
            "F1 Best 6\n",
            "Accuracy Best 7\n",
            "F1 Best 7\n",
            "Accuracy Best 8\n",
            "Accuracy Best 9\n",
            "Accuracy Best 10\n",
            "Accuracy Best 12\n",
            "Accuracy Best 13\n",
            "Accuracy Best 14\n",
            "Accuracy Best 17\n",
            "Accuracy Best 18\n",
            "Accuracy Best 20\n",
            "Accuracy Best 24\n",
            "Accuracy Best 28\n",
            "Accuracy Best 31\n",
            "Accuracy Best 32\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-932c99fca1b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_Val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_Val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_reduced\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_Val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_Val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1587\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"processes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m         fold_coefs_ = Parallel(\n\u001b[0m\u001b[1;32m   1590\u001b[0m             \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    804\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             ]\n\u001b[0;32m--> 806\u001b[0;31m             opt_res = optimize.minimize(\n\u001b[0m\u001b[1;32m    807\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                   **options)\n\u001b[1;32m    622\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    624\u001b[0m                                 callback=callback, **options)\n\u001b[1;32m    625\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_loss_and_grad\u001b[0;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mz0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "data_pd_cp = data_pd.copy()\n",
        "\n",
        "# Standardize the data using z-scores\n",
        "data_z = data_pd_cp.apply(zscore)\n",
        "\n",
        "# Calculate the absolute z-score for each data point\n",
        "data_z_abs = data_z.abs()\n",
        "\n",
        "# Identify the outliers based on a z-score threshold\n",
        "outliers = (data_z_abs > 3).any(axis=1)\n",
        "\n",
        "# Print the indices of the outliers\n",
        "outlier_indices = (outliers.index[outliers])\n",
        "\n",
        "data_pd_cp = data_pd_cp.drop(outlier_indices)\n",
        "\n",
        "# Extract the features (x) and target variable (y) from the dataset\n",
        "x = data_pd_cp.drop('Target1', axis=1)\n",
        "y = data_pd_cp['Target1']\n",
        "\n",
        "# Initialize the StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the features and transform the data\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "\n",
        "pca = PCA(n_components = 65)\n",
        "data_reduced = pca.fit_transform(x_scaled)\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "model.fit(data_reduced,y)\n",
        "\n",
        "# X is your feature matrix and y is your target vector\n",
        "# model is your logistic regression model\n",
        "scores = cross_val_score(model, data_reduced, y, cv=5)\n",
        "\n",
        "# Print the accuracy for each fold of cross-validation\n",
        "print(\"Accuracy for each fold:\", scores)\n",
        "\n",
        "# Print the mean accuracy across all folds\n",
        "print(\"Mean accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN-LaFO9LsiD",
        "outputId": "2ce168e3-f9fe-4b37-cb62-a6d37ee52ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for each fold: [0.69337792 0.70840558 0.71410288 0.70906614 0.69238709]\n",
            "Mean accuracy: 0.7034679217240525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(model, data_reduced, y, cv=10)\n",
        "\n",
        "# Print the accuracy for each fold of cross-validation\n",
        "print(\"Accuracy for each fold:\", scores)\n",
        "\n",
        "# Print the mean accuracy across all folds\n",
        "print(\"Mean accuracy:\", scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIB8meTzUvlj",
        "outputId": "cf2c3568-580b-4c97-f632-357b03b4c14f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for each fold: [0.69185038 0.69961192 0.70415325 0.71373132 0.71629098 0.71166708\n",
            " 0.70729089 0.7094377  0.70167616 0.68235488]\n",
            "Mean accuracy: 0.7038064569399719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data_pd_cp = data_pd.copy()\n",
        "\n",
        "# Standardize the data using z-scores\n",
        "data_z = data_pd_cp.apply(zscore)\n",
        "\n",
        "# Calculate the absolute z-score for each data point\n",
        "data_z_abs = data_z.abs()\n",
        "\n",
        "# Identify the outliers based on a z-score threshold\n",
        "outliers = (data_z_abs > 3).any(axis=1)\n",
        "\n",
        "# Print the indices of the outliers\n",
        "outlier_indices = (outliers.index[outliers])\n",
        "\n",
        "data_pd_cp = data_pd_cp.drop(outlier_indices)\n",
        "\n",
        "# Extract the features (x) and target variable (y) from the dataset\n",
        "x = data_pd_cp.drop('Target1', axis=1)\n",
        "y = data_pd_cp['Target1']\n",
        "\n",
        "# Initialize the StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the features and transform the data\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "# Assuming y_test and y_pred are the true labels and predicted labels for the test set, respectively, for two models\n",
        "pca = PCA(n_components = 65)\n",
        "data_reduced = pca.fit_transform(x_scaled)\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_reduced,y, train_size=0.8,random_state=42)\n",
        "model.fit(X_train,y_train)\n",
        "y_pred_model1 = model.predict(X_test)\n",
        "\n",
        "pca = PCA(n_components = 2)\n",
        "data_reduced2 = pca.fit_transform(x_scaled)\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(data_reduced2,y, train_size=0.8,random_state=42)\n",
        "model.fit(X_train1,y_train1)\n",
        "y_pred_model2 = model.predict(X_test1)\n",
        "\n",
        "\n",
        "# Calculate the fpr, tpr, and threshold values for the two models\n",
        "fpr1, tpr1, threshold1 = roc_curve(y_test, y_pred_model1)\n",
        "fpr2, tpr2, threshold2 = roc_curve(y_test1, y_pred_model2)\n",
        "\n",
        "# Plot the ROC curves for the two models\n",
        "plt.plot(fpr1, tpr1, label='Model 1')\n",
        "plt.plot(fpr2, tpr2, label='Model 2')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "\n",
        "# Calculate the AUC values for the two models\n",
        "auc1 = roc_auc_score(y_test, y_pred_model1)\n",
        "auc2 = roc_auc_score(y_test1, y_pred_model2)\n",
        "\n",
        "# Compare the AUC values for the two models\n",
        "print('AUC for Model 1:', auc1)\n",
        "print('AUC for Model 2:', auc2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "JiiyTS8EVxci",
        "outputId": "d7041756-ce32-40c7-b18a-6884ff68a8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC for Model 1: 0.6911657486558257\n",
            "AUC for Model 2: 0.579598115202955\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}